{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmilisGit/Deep_learning/blob/main/lab2_poetry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install markovify -q\n",
        "!pip install num2words -q\n",
        "!pip install pronouncing -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LaT_w6Dz6y-",
        "outputId": "2a57a5a8-5fee-443a-f543-7184b5aa0e72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pronouncing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MJ3hHF_xAki1"
      },
      "outputs": [],
      "source": [
        "import os, re, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "import markovify\n",
        "import kagglehub\n",
        "from num2words import num2words\n",
        "import pronouncing\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.set_random_seed(812)\n",
        "random.seed(812)"
      ],
      "metadata": {
        "id": "FDx_weZIvVjW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Duomenų atsiuntimas ir apžiūra"
      ],
      "metadata": {
        "id": "E84GDfs8ukLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datapath = kagglehub.dataset_download(\"paultimothymooney/poetry\")\n",
        "data_files = os.listdir(datapath)\n",
        "print(f'Downloaded {len(data_files)} files:', data_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKDeRniyumot",
        "outputId": "29a6ebb3-8232-469c-a933-2d71f9848501"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/paultimothymooney/poetry?dataset_version_number=16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.00M/2.00M [00:00<00:00, 103MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Downloaded 49 files: ['bob-dylan.txt', 'kanye-west.txt', 'beatles.txt', 'lil-wayne.txt', 'notorious_big.txt', 'nirvana.txt', 'eminem.txt', 'michael-jackson.txt', 'alicia-keys.txt', 'nicki-minaj.txt', 'blink-182.txt', 'disney.txt', 'rihanna.txt', 'r-kelly.txt', 'leonard-cohen.txt', 'nursery_rhymes.txt', 'notorious-big.txt', 'bob-marley.txt', 'missy-elliott.txt', 'lin-manuel-miranda.txt', 'dolly-parton.txt', 'cake.txt', 'kanye.txt', 'bruno-mars.txt', 'amy-winehouse.txt', 'dickinson.txt', 'bieber.txt', 'janisjoplin.txt', 'prince.txt', 'bjork.txt', 'britney-spears.txt', 'dr-seuss.txt', 'adele.txt', 'Lil_Wayne.txt', 'lorde.txt', 'bruce-springsteen.txt', 'joni-mitchell.txt', 'jimi-hendrix.txt', 'paul-simon.txt', 'nickelback.txt', 'Kanye_West.txt', 'dj-khaled.txt', 'drake.txt', 'radiohead.txt', 'lady-gaga.txt', 'al-green.txt', 'johnny-cash.txt', 'ludacris.txt', 'patti-smith.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Funkcijos\n",
        "def normalize_word_line(line: str) -> list[str]:\n",
        "  row = [x.lower() for x in re.findall(r\"\\w+'?\\w*\", line)]\n",
        "  new_row = []\n",
        "  for word in row:\n",
        "    numbers = re.findall(r\"\\d+\", word)\n",
        "    for n in numbers:\n",
        "      word = word.replace(n, num2words(int(n)))\n",
        "    new_row.append(word)\n",
        "  return new_row"
      ],
      "metadata": {
        "id": "zt9z0Kyc_8X4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ritmo išgavimas\n",
        "def n_syllables(word_line: list[str]) -> int:\n",
        "    vowels = 'aeiouy'\n",
        "    syllable_count = 0\n",
        "\n",
        "    for word in word_line:\n",
        "        for i, char in enumerate(word):\n",
        "            if char in vowels:\n",
        "                if (i == 0) or (word[i-1] not in vowels):\n",
        "                    syllable_count += 1\n",
        "        word_vowels_count = sum([x in vowels for x in word])\n",
        "        if word_vowels_count == 0:  # abreviaturos\n",
        "            syllable_count = len(word)  # pvz. \"NLP\" skaitosi kaip \"en-el-pi\"\n",
        "        elif word_vowels_count > 1 and (word[-1] == 'e') and (word[-2] not in vowels):  # paskutinė \"e\" dažnai nesakoma anglų k., bet \"ie\", \"ee\" ištariama\n",
        "            syllable_count -= 1\n",
        "\n",
        "    return syllable_count\n",
        "\n",
        "def get_rhyme(line: list[str]) -> str:\n",
        "    last_word = re.sub('\\W+', '', line[-1])\n",
        "    all_rhymes = pronouncing.rhymes(last_word)\n",
        "    if all_rhymes:\n",
        "        rhyming_ends = [x[-2:] for x in all_rhymes]\n",
        "        most_common_rhyme = max(set(rhyming_ends), key=rhyming_ends.count)\n",
        "    else:\n",
        "        most_common_rhyme = last_word[-2:]\n",
        "    return most_common_rhyme\n",
        "\n",
        "\n",
        "def get_rhyme_list(normalized_lyrics: list[list[str]]):\n",
        "  rhyme_set = set()\n",
        "  for row in normalized_lyrics:\n",
        "    most_common_rhyme = get_rhyme(row)\n",
        "    rhyme_set.add(most_common_rhyme)\n",
        "\n",
        "  sorted_rhyme_set = sorted(list(rhyme_set), key=lambda x: x[-1])\n",
        "  return sorted_rhyme_set"
      ],
      "metadata": {
        "id": "pGA101F393i1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"n_syllables output: \", n_syllables([\"good day everyone\"]))\n",
        "print(\"get_rhyme output: \", get_rhyme([\"good day everyone\"]))\n",
        "print(get_rhyme_list([[\"that's the strat\"], [\"make it fast\"]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfmOy0Fp5mEP",
        "outputId": "84f1612b-edab-4f5d-c37a-8ca12518a1b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_syllables output:  5\n",
            "get_rhyme output:  ne\n",
            "['st', 'at']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Duomenų rinkinio paruošimas\n",
        "\n",
        "Modelis negalės tiesiogiai operuoti skiemenimis, todėl turime naudoti skaičius:"
      ],
      "metadata": {
        "id": "hqrqD4NVfQvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Duomenų rinkinio paruošimas\n",
        "def get_rhyme_float(line: list[str], rhyme_list: list[str]) -> float | None:\n",
        "  rhyme = get_rhyme(line)\n",
        "  if rhyme in rhyme_list:\n",
        "    return rhyme_list.index(rhyme) / len(rhyme_list)\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def length_test(sentence):\n",
        "    max_words = 8\n",
        "    return len(sentence.split()) <= max_words\n",
        "\n",
        "\n",
        "def get_random_lines(markov_model, n_rows: int) -> list[list[str]]:\n",
        "  lines = []\n",
        "  last_words = []\n",
        "\n",
        "  while len(lines) < n_rows:\n",
        "    line = markov_model.make_short_sentence(max_chars=40)\n",
        "    # nenorime gauti tuščios eilutės ar jau turimos eilutės\n",
        "    if (line is not None) and (line not in lines):\n",
        "      last_word = normalize_word_line(line)[-1]\n",
        "      # nenorime kad dažnai pasikartotų tas pats žodis eilutės gale\n",
        "      if last_words.count(last_word) < 3:\n",
        "        lines.append(normalize_word_line(line))\n",
        "        last_words.append(last_word)\n",
        "\n",
        "  return lines\n",
        "\n",
        "\n",
        "def get_line_features(line: list[str], rhyme_list: list[str]) -> tuple:\n",
        "  return (line, n_syllables(line), get_rhyme_float(line, rhyme_list))\n",
        "\n",
        "def build_dataset(lines: list[list[str]], rhyme_list: list[str]):\n",
        "\tfeatures = [get_line_features(x, rhyme_list) for x in lines]\n",
        "\tx_data, y_data = [], []\n",
        "\n",
        "  # turėsime standartinę struktūrą kai eilutės rimuojasi po 4 grupėje\n",
        "\t# pirmos dvi eilutės bus pradinės savybės, antros dvi eilutės - prognozuojamos\n",
        "\tfor i in range(len(features) - 3):\n",
        "\t\t# duomenyse liks tik eilučių savybes, todėl visur [1:]\n",
        "\t\tline1, line2 = features[i    ][1:], features[i + 1][1:]\n",
        "\t\tline3, line4 = features[i + 2][1:], features[i + 3][1:]\n",
        "\t\tx_data.append(np.array([line1, line2]))\n",
        "\t\ty_data.append(np.array([line3, line4]))\n",
        "\treturn np.array(x_data), np.array(y_data)"
      ],
      "metadata": {
        "id": "EVXYruyxSRwG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. RNN modelio inicializavimas\n",
        "\n",
        "Mūsų modelis turės prognozuoti naujų 4 eilučių savybes gaunant senas 4 eilutes."
      ],
      "metadata": {
        "id": "M65DDnrB1wa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lstm(depth: int):\n",
        "\n",
        "  keras.backend.clear_session()  # pašaliname tarpinių modelių likučius\n",
        "  keras.utils.set_random_seed(812)\n",
        "\n",
        "  model = keras.Sequential(name='LSTM-based_lyrics_generator')\n",
        "  model.add(keras.layers.Input((2, 2)))\n",
        "  model.add(keras.layers.LSTM(16, return_sequences=True))\n",
        "  for i in range(depth):\n",
        "    model.add(keras.layers.LSTM(16, return_sequences=True))\n",
        "  model.add(keras.layers.LSTM(2, return_sequences=True))\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "      loss='mse')\n",
        "\n",
        "  return model\n",
        "\n",
        "model = create_lstm(depth=2)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "oAMK9WWt11F-",
        "outputId": "029ce1b3-0404-44e3-8b6d-7b5ff820163d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"LSTM-based_lyrics_generator\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM-based_lyrics_generator\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)               │           \u001b[38;5;34m1,216\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)               │           \u001b[38;5;34m2,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)               │           \u001b[38;5;34m2,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)                │             \u001b[38;5;34m152\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">152</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,592\u001b[0m (21.84 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,592</span> (21.84 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,592\u001b[0m (21.84 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,592</span> (21.84 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dviejų modelių sistema\n",
        "def compose(starting_input: np.ndarray, rnn_model, n_line_groups: int):\n",
        "\tfinal_vectors = []\n",
        "\tstarting_vectors = rnn_model.predict(starting_input).flatten().reshape(1, 2, 2)\n",
        "\tfinal_vectors.append(starting_vectors)\n",
        "\tfor i in range(n_line_groups):\n",
        "\t\tprev_vectors = final_vectors[-1]\n",
        "\t\tfinal_vectors.append(rnn_model.predict(prev_vectors).flatten().reshape(1, 2, 2))\n",
        "\treturn final_vectors\n",
        "\n",
        "\n",
        "def last_word_compare(prev_lines: list[list[str]], new_line: list[str], penalty: float = 0.2) -> float:\n",
        "\tsum_penalty = 0.0\n",
        "\tfor line in prev_lines:\n",
        "\t\tif line[-1] == new_line[-1]:\n",
        "\t\t\tsum_penalty += penalty\n",
        "\treturn sum_penalty\n",
        "\n",
        "\n",
        "def calculate_score(features, n_syllables, rhyme, penalty: float, rhyme_list, maxsyllables):\n",
        "\tdesired_n_syllables = features[0] * maxsyllables\n",
        "\tdesired_rhyme = features[1] * len(rhyme_list)\n",
        "\tsyllable_score = - abs(float(desired_n_syllables) - float(n_syllables))\n",
        "\trhyme_score = 2.0 * abs(float(desired_rhyme) - float(rhyme))\n",
        "\tscore = 1.0 + syllable_score + rhyme_score - penalty\n",
        "\treturn score\n",
        "\n",
        "\n",
        "def vectors_into_song(vectors, generated_lyrics, rhyme_list, maxsyllables: int):\n",
        "\tsong = []\n",
        "\tgenerated_features = [get_line_features(x, rhyme_list) for x in generated_lyrics]\n",
        "\n",
        "\tvector_halves = []\n",
        "\tfor vector in vectors:\n",
        "\t\tvector_halves.extend(vector[0].tolist())\n",
        "\n",
        "\tfor vector in vector_halves:\n",
        "\t\tscorelist = []\n",
        "\n",
        "\t\tfor (line, n_syllables, rhyme) in generated_features:\n",
        "\t\t\tif len(song) != 0:\n",
        "\t\t\t\tpenalty = last_word_compare(song, line)\n",
        "\t\t\telse:\n",
        "\t\t\t\tpenalty = 0\n",
        "\n",
        "\t\t\ttotal_score = calculate_score(vector, n_syllables, rhyme, penalty, rhyme_list, maxsyllables)\n",
        "\t\t\tscorelist.append([line, total_score])\n",
        "\n",
        "\t\t# randame eilutę su aukščiausiu įvertinimu\n",
        "\t\tbest_line_index = np.argmax([float(x[1]) for x in scorelist])\n",
        "\t\tbest_line = scorelist[best_line_index][0]\n",
        "\t\tsong.append(best_line)\n",
        "\n",
        "\t\t# pašaliname šią eilutę iš likusių eilučių sąrašo\n",
        "\t\tgenerated_features = [x for x in generated_features if x[0] != best_line]\n",
        "\n",
        "\treturn [' '.join(x) for x in song]"
      ],
      "metadata": {
        "id": "HtOIBhYm6Sfr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "class LyricsGenerator:\n",
        "    def __init__(self, lstm_model, markov_model, lyrics, maxsyllables=12):\n",
        "        self.lstm_model = lstm_model\n",
        "        self.markov_model = markov_model\n",
        "        self.rhyme_list = self.get_rhyme_list(lyrics)\n",
        "        self.maxsyllables = maxsyllables\n",
        "        self.x_data = None\n",
        "        self.y_data = None\n",
        "\n",
        "    def n_syllables(self, word_line: list[str]) -> int:\n",
        "        vowels = 'aeiouy'\n",
        "        syllable_count = 0\n",
        "\n",
        "        for word in word_line:\n",
        "            for i, char in enumerate(word):\n",
        "                if char in vowels:\n",
        "                    if (i == 0) or (word[i-1] not in vowels):\n",
        "                        syllable_count += 1\n",
        "            word_vowels_count = sum([x in vowels for x in word])\n",
        "            if word_vowels_count == 0:  # abreviaturos\n",
        "                syllable_count = len(word)  # pvz. \"NLP\" skaitosi kaip \"en-el-pi\"\n",
        "            elif word_vowels_count > 1 and (word[-1] == 'e') and (word[-2] not in vowels):  # paskutinė \"e\" dažnai nesakoma anglų k., bet \"ie\", \"ee\" ištariama\n",
        "                syllable_count -= 1\n",
        "\n",
        "        return syllable_count\n",
        "\n",
        "\n",
        "    def get_rhyme_float(self, line: list[str]) -> float | None:\n",
        "      rhyme = self.get_rhyme(line)\n",
        "      if rhyme in self.rhyme_list:\n",
        "        return self.rhyme_list.index(rhyme) / len(self.rhyme_list)\n",
        "      else:\n",
        "        return None\n",
        "\n",
        "    def length_test(self, sentence):\n",
        "        max_words = 8\n",
        "        return len(sentence.split()) <= max_words\n",
        "\n",
        "\n",
        "    def get_line_features(self, line: list[str]) -> tuple:\n",
        "      return (line, self.n_syllables(line), self.get_rhyme_float(line))\n",
        "\n",
        "    def build_dataset(self, lines: list[list[str]]):\n",
        "      features = [self.get_line_features(x) for x in lines]\n",
        "      x_data, y_data = [], []\n",
        "\n",
        "      # turėsime standartinę struktūrą kai eilutės rimuojasi po 4 grupėje\n",
        "      # pirmos dvi eilutės bus pradinės savybės, antros dvi eilutės - prognozuojamos\n",
        "      for i in range(len(features) - 3):\n",
        "        # duomenyse liks tik eilučių savybes, todėl visur [1:]\n",
        "        line1, line2 = features[i    ][1:], features[i + 1][1:]\n",
        "        line3, line4 = features[i + 2][1:], features[i + 3][1:]\n",
        "        x_data.append(np.array([line1, line2]))\n",
        "        y_data.append(np.array([line3, line4]))\n",
        "      self.x_data = np.array(x_data)\n",
        "      self.y_data = np.array(y_data)\n",
        "\n",
        "    def get_rhyme(self, line: list[str]) -> str:\n",
        "      last_word = re.sub('\\W+', '', line[-1])\n",
        "      all_rhymes = pronouncing.rhymes(last_word)\n",
        "      if all_rhymes:\n",
        "          rhyming_ends = [x[-2:] for x in all_rhymes]\n",
        "          most_common_rhyme = max(set(rhyming_ends), key=rhyming_ends.count)\n",
        "      else:\n",
        "          most_common_rhyme = last_word[-2:]\n",
        "      return most_common_rhyme\n",
        "\n",
        "    def normalize_word_line(self, line: str) -> list[str]:\n",
        "      row = [x.lower() for x in re.findall(r\"\\w+'?\\w*\", line)]\n",
        "      new_row = []\n",
        "      for word in row:\n",
        "        numbers = re.findall(r\"\\d+\", word)\n",
        "        for n in numbers:\n",
        "          word = word.replace(n, num2words(int(n)))\n",
        "        new_row.append(word)\n",
        "      return new_row\n",
        "\n",
        "    def get_rhyme_list(self, normalized_lyrics: list[list[str]]):\n",
        "      rhyme_set = set()\n",
        "      for row in normalized_lyrics:\n",
        "        most_common_rhyme = get_rhyme(row)\n",
        "        rhyme_set.add(most_common_rhyme)\n",
        "\n",
        "      sorted_rhyme_set = sorted(list(rhyme_set), key=lambda x: x[-1])\n",
        "      return sorted_rhyme_set\n",
        "\n",
        "    def compose(self, starting_input, n_line_groups):\n",
        "        final_vectors = []\n",
        "        starting_vectors = self.lstm_model.predict(starting_input).flatten().reshape(1, 2, 2)\n",
        "        final_vectors.append(starting_vectors)\n",
        "        for i in range(n_line_groups):\n",
        "            prev_vectors = final_vectors[-1]\n",
        "            final_vectors.append(self.lstm_model.predict(prev_vectors).flatten().reshape(1, 2, 2))\n",
        "        return final_vectors\n",
        "\n",
        "    def last_word_compare(self, prev_lines, new_line, penalty=0.2):\n",
        "        sum_penalty = 0.0\n",
        "        for line in prev_lines:\n",
        "            if line[-1] == new_line[-1]:\n",
        "                sum_penalty += penalty\n",
        "        return sum_penalty\n",
        "\n",
        "    def calculate_score(self, features, n_syllables, rhyme, penalty):\n",
        "        desired_n_syllables = features[0] * self.maxsyllables\n",
        "        desired_rhyme = features[1] * len(self.rhyme_list)\n",
        "        syllable_score = -abs(desired_n_syllables - n_syllables)\n",
        "        rhyme_score = 2.0 * abs(desired_rhyme - rhyme)\n",
        "        return 1.0 + syllable_score + rhyme_score - penalty\n",
        "\n",
        "    def vectors_into_song(self, vectors, generated_lyrics):\n",
        "        song = []\n",
        "        generated_features = [get_line_features(x, self.rhyme_list) for x in generated_lyrics]\n",
        "        vector_halves = []\n",
        "        for vector in vectors:\n",
        "            vector_halves.extend(vector[0].tolist())\n",
        "        for vector in vector_halves:\n",
        "            scorelist = []\n",
        "            for (line, n_syllables, rhyme) in generated_features:\n",
        "                penalty = self.last_word_compare(song, line) if song else 0\n",
        "                total_score = self.calculate_score(vector, n_syllables, rhyme, penalty)\n",
        "                scorelist.append([line, total_score])\n",
        "            best_line_index = np.argmax([float(x[1]) for x in scorelist])\n",
        "            best_line = scorelist[best_line_index][0]\n",
        "            song.append(best_line)\n",
        "            generated_features = [x for x in generated_features if x[0] != best_line]\n",
        "        return [' '.join(x) for x in song]\n",
        "\n",
        "    def get_random_lines(self, markov_model, n_rows: int) -> list[list[str]]:\n",
        "      lines = []\n",
        "      last_words = []\n",
        "\n",
        "      while len(lines) < n_rows:\n",
        "        line = markov_model.make_short_sentence(max_chars=40)\n",
        "        # nenorime gauti tuščios eilutės ar jau turimos eilutės\n",
        "        if (line is not None) and (line not in lines):\n",
        "          last_word = self.normalize_word_line(line)[-1]\n",
        "          # nenorime kad dažnai pasikartotų tas pats žodis eilutės gale\n",
        "          if last_words.count(last_word) < 3:\n",
        "            lines.append(self.normalize_word_line(line))\n",
        "            last_words.append(last_word)\n",
        "\n",
        "      return lines\n",
        "\n",
        "    def generate_song(self, start, num_lines=200, n_line_groups=4):\n",
        "        vectors = self.compose(start, n_line_groups)\n",
        "        some_lyrics = self.get_random_lines(self.markov_model, num_lines)\n",
        "        return self.vectors_into_song(vectors, some_lyrics)\n"
      ],
      "metadata": {
        "id": "0Nn79bnbMp8m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sudedame viską į vientą vietą:"
      ],
      "metadata": {
        "id": "lAcjfK-zfNJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/Colab/NLP_ND/LSTM-based_lyrics_generator.keras')\n",
        "\n",
        "artist_files = ['nursery_rhymes.txt', 'drake.txt']\n",
        "raw_lyrics = \"\"\n",
        "for file in artist_files:\n",
        "  with open(os.path.join(datapath, file), 'r') as f:\n",
        "      contents = f.read()\n",
        "      raw_lyrics += contents + \"\\n\"\n",
        "\n",
        "markov_model = markovify.NewlineText(raw_lyrics)\n",
        "\n",
        "lyrics = [normalize_word_line(x) for x in raw_lyrics.splitlines()]\n",
        "lyrics = [x for x in lyrics if x]\n",
        "generator = LyricsGenerator(model, markov_model, lyrics)\n",
        "generator.generate_song()"
      ],
      "metadata": {
        "id": "yKSgQgX9NApl",
        "outputId": "97abd6ac-8239-40b9-c988-7a45c1213283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data, y_data = build_dataset(lyrics, generator.rhyme_list)\n",
        "start_i = np.random.choice(range(len(x_data)))\n",
        "start = np.array([x_data[start_i]])\n",
        "start"
      ],
      "metadata": {
        "id": "ADnb_mn6RBwb",
        "outputId": "f0957a7f-f5af-4e84-d34f-9a020872a78b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[8.        , 0.87684729],\n",
              "        [8.        , 0.4679803 ]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "artist_files = ['nursery_rhymes.txt', 'drake.txt']\n",
        "raw_lyrics = \"\"\n",
        "for file in artist_files:\n",
        "  with open(os.path.join(datapath, file), 'r') as f:\n",
        "      contents = f.read()\n",
        "      raw_lyrics += contents + \"\\n\"\n",
        "\n",
        "markov_model = markovify.NewlineText(raw_lyrics)\n",
        "\n",
        "lyrics = [normalize_word_line(x) for x in raw_lyrics.splitlines()]\n",
        "lyrics = [x for x in lyrics if x]\n",
        "rhymes = get_rhyme_list(lyrics)\n",
        "\n",
        "x_data, y_data = build_dataset(lyrics, rhymes)\n",
        "\n",
        "model = create_lstm(depth=4)\n",
        "print(model.summary())\n",
        "\n",
        "model.fit(\n",
        "    x_data, y_data,\n",
        "    batch_size=16,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model.save('/content/drive/MyDrive/Colab/LSTM-based_lyrics_generator.keras')\n",
        "\n",
        "\n",
        "start_i = np.random.choice(range(len(x_data)))\n",
        "start = np.array([x_data[start_i]])\n",
        "vectors = compose(start, model, 4)\n",
        "some_lyrics = get_random_lines(markov_model, 2000)\n",
        "vectors_into_song(vectors, some_lyrics, rhymes, maxsyllables=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "esKvto25lXlL",
        "outputId": "0f0730e9-c9bc-42bb-98b6-4c3bb62123d1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"LSTM-based_lyrics_generator\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"LSTM-based_lyrics_generator\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)               │           \u001b[38;5;34m1,216\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)               │           \u001b[38;5;34m2,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)               │           \u001b[38;5;34m2,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)               │           \u001b[38;5;34m2,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m16\u001b[0m)               │           \u001b[38;5;34m2,112\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)                │             \u001b[38;5;34m152\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,216</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">152</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,816\u001b[0m (38.34 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,816</span> (38.34 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,816\u001b[0m (38.34 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,816</span> (38.34 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/10\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 47.6122\n",
            "Epoch 2/10\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - loss: 44.1683\n",
            "Epoch 3/10\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 44.1670\n",
            "Epoch 4/10\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 44.1669\n",
            "Epoch 5/10\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 44.1669\n",
            "Epoch 6/10\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 44.1669\n",
            "Epoch 7/10\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 44.1669\n",
            "Epoch 8/10\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 44.1669\n",
            "Epoch 9/10\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 44.1669\n",
            "Epoch 10/10\n",
            "\u001b[1m483/483\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 44.1669\n",
            "Mounted at /content/drive\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['instead of the judah',\n",
              " 'you know what it was in the mia',\n",
              " 'put her in the mafia',\n",
              " 'the hare she loves the high wood',\n",
              " 'felt like the titanic',\n",
              " 'i try not to give to the hood',\n",
              " 'couple days on the music',\n",
              " 'the things you say you love so bad',\n",
              " \"and i ain't asthmatic\",\n",
              " 'this is more than i ever had']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}